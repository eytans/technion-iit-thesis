\section{Evaluation}
\label{results}

Evaluating a theory exploration system poses some challenges.
One challenge is that different combinations of lemmas can be used to express the same knowledge (in the sense that was described in \autoref{overview}).
Testing whether a discovered theory expresses the correct and full knowledge expected could be done by checking that the knowledge obtained from exploration subsumes the expected knowledge. 
However, this check is not feasible because, essentially, $\mathcal{K}_\mathcal{T}^S$ is infinite; notice that it is not sufficient to check whether all of the lemmas in some $\mathcal{T}_1$ can be proven using some other $\mathcal{T}_2$. There may still be some formula $\varphi$ such that $\mathcal{T}_1 \vdash^S \varphi$ but  $\mathcal{T}_2 \not\vdash^S \varphi$ --- due to bounds necessarily imposed on proof search in order to guarantee termination.
For this reason, we resort to a manual investigation of the result relative to a set of known lemmas that we expect to be found (for example, $l_1\concat {[]} = l_1$).
Another challenge arises when comparing with existing tools for automated reasoning since different tools work in different scenarios in which they expect different kinds of user input.
As a principal, our design goals for \TheSy was to perform exploration using function definitions alone without additional user guidance.
This affects the interpretation of any comparison to theorem provers or verification systems such as Leon, as the input is meaningfully different.
To evaluate \TheSy we compare with Hipster~\cite{hipster}, a fully automated exploration system with several follow-ups \cite{hipstercond,ITP2017:Johansson,2018AISC:Einarsdottir}.

As a comparison of \TheSy and Hipster requires manual analysis, we perform a case study of a handful of test cases that we consider representative, while adding a discussion of the results for each of them and how they relate.
Besides that, all lemmas found during exploration are inserted to Leon to attempt to verify them.
This last step is also a demonstration of how theory exploration can strengthen and augment software verification.

We present the implementation details and experimental setup, then move to showcase the capabilities of \TheSy using a collection of benchmarks that is representative of functional programming practices. These are shown independently of preexisting tools.
This is followed by a discussion of the comparison with other systems.

\subsection{Implementation and Setup}
\label{results:implementation}

As \TheSy relies heavily on term rewriting, there are a few additions to our implementation of the PEG data structure to improve performance.
The rewriting engine is incremental, relying on the PEG supporting incremental processing:
Our incremental PEG keeps track of when edges are added, and during the pattern match step of rewrite rule application, at least one matched edge should be new; that is, it has been created after the last time that rule was applied.
This optimization avoids recomputation of pattern matching across the ever-growing hypergraph, and prevents unnecessary edge insertions and node merges.

\begin{comment}
When running SOE, all the different symbolic valuations are inserted into the hypergraph at once, 
and the hypergraph is duplicated for each valuation to keep terms on different valuations distinct.
By combining the duplicated hypergraphs into one, there is sharing of the symbolic term evaluation for all valuations.
We reuse uninterpreted values when creating larger symbolic expressions (actually, the whole example is reused). 
By which taking advantage of the term sharing to reduce the amount of rewriting needed.

For example consider the valuations for list $[], [x], [y,x]$, both $[]$ and $[x]$ are reused.
As a result, when evaluating a term, for example $rev$, less steps are needed as it reuses the calculations for the smaller valuation.
\[ rev([]) \rwto [] \quad rev([x]) \rwto rev([]) :+ x \quad rev([y,x]) \rwto rev([x]) :+ y \]
\end{comment}

\begin{comment}
\TheSy also applies different rewrite rules in parallel, providing some additional speedup.
This speedup is vital when the knowledge base grows, meaning more lemmas exist, as each lemma gets translated into a rewrite rule.
By matching and concluding in parallel, rules that are irrelevant to the current context fail to match instantly and do not slow down execution.
The compaction operation, however, is sequential, and it currently dominates the running time.
There is definitely room for further implementation improvements to accelerate the performance-critical parts.
\end{comment}

As mentioned, \TheSy supports automatic case splitting; this is done for functions that contain $\tmatch$ expressions ($\tif$ is just a special case of $\tmatch$). 
A special rewrite rule is created to enable split cases during term evaluation, by marking terms to split and split values. 
For each split marker and relating split values, it copies the hypergraph for each split value, replacing the marked term with the split value. 
Case splitting is done recursively, performing rewrite search on each split value for each level until reaching the maximum split depth. 
After finishing the search for all branches, conclusions are merged to find what equalities were proved.
This implementation results in repeating the same branches due to the ordering, which worsens with deeper split depth. 
As there is no consideration of whether a case split will be useful, depending on where it is applied, many unnecessary case splitting is done.
While limiting scale in its current format, \TheSy was able to find lemmas about the functional operator `$\tfilter$' with small vocabularies.

\TheSy has a few control parameters governing the term generation and search processes.
For experiments in which we use non-default parameters, the decision is explained in the following discussion.
Now we review the parameters and their impact on the exploration results.
The rewrite search depth $d$ affects what equalities we might find: A shallow search can lead to missed lemmas,
but deeper search may be prohibitively expensive as some of the rewrite rules can run infinitely, such as in the case of $x \rightarrow x + 0$.
The default search depth is $d = 8$. 
Terms are always generated up to depth 2 ($k = 2$).
The symbolic expressions used for discriminating terms are created with a maximal depth of 2 ($c = 2$).
When using the case split feature, larger symbolic expressions will require increasing the maximum split depth, therefore in those cases, $c$ is reduced to 1.
(This reduction negatively impacts the robustness of the inference phase; \eg, $\treverse~l_1=l_1$ for lists of size 1. These spurious lemmas will have to be discarded by the induction prover.)

All benchmarks were executed on a Xeon E3-1231 v3 (quad-core) machine with 32GB of RAM.

\subsection{Experimental Results}
\label{results:experimental}

The cases being checked are defined by the sorted vocabulary used, which encapsulates the relevant functions and data type definitions.
For each experiment, we report the number of lemmas which were: found and proven, passed screening but failed the proof, and those for which proof succeeded only on retry.
If a proof failed when a lemma was first discovered, but succeeded later on, it means that a necessary auxiliary lemma was found and that lemma enabled the proof to go through.
This situation is common as auxiliary lemmas are sometimes larger and occur later in our ordering.

Some of the lemmas proven in early test cases are reused later to prevent \TheSy from reproving lemmas. 
Lemmas for $\lor$ and $\land$, which do not require induction to prove, are also encoded as rewrite rules.
(\TheSy can in fact prove them, but they are discarded during screening for being to simple.)

The experimental results are listed in \autoref{eval:all-lemmas}, where for each given vocabulary all the lemmas found by TheSy are presented. Additionally a breakdown of the run time is presented in \autoref{table:experiments}. As the term generation and conjecture inference phases are highly coupled and as the term generation is relatively insignificant, the total time for both phases is presented.
The conjecture screening phase happens many times, as it is needed when proofs are retried.
We therefore seperate its run time from the prover, presenting the accumulated time spent screening.
The prover column includes two measurements. 
The left one is the total time spent on in the prover.
The left represent the amount of time, from the total time in the prover, spent on retrying to prove a conjecture.
All the measurements are given in seconds.

There are two measurements given for experiments containing $\tfold$ in the vocabulary.
When using the default parameters, \TheSy creates many unneeded terms for those cases.
To demonstrate the impact of this, we added a second run, reducing the number of placeholder values created.
Certainly, it is possible to optimize all experiments by choosing the correct parameters, but the goal remains to work fully automatically therefor it is preferred to keep the defaults.

Another design choice impacting performance is adding a conjecture filtering phase, in the end of the iterative deepening, after finding new rules.
This is important as was shown in the end of the overview (\autoref{overview}), as it allows finding new lemmas.
The additional knowledge, discovered during the exploration, helps the conjecture inference phase discover more equalities.
The effect on run-time can be astronomical when working with larger vocabularies.
This is because continued rewriting of large hypergraphs becomes resource expensive as they keep growing.
For example, considering the stress test, finding the lemma -- $\tmap~f~(\trev~l) = \trev~(\tmap~f~l)$ -- is possible due to this additional run, which changed the run-time from ~1 hour to ~4.
Still, when considering theory exploration, the additional lemma can be very useful for the future and should not be overlooked.

A few concepts are displayed in the experimentation.
\begin{itemize}
    \item Being able to find basic facts about new data types such as Nat, List, and Tree.
    \item The ability to explore higher-kind functions such as filter map and fold.
    \item Working with bigger vocabularies as seen in the stress test
    \item Knowledge transfer, some of the experiments successfully reuse knowledge to find new results (this can be seen in the "filter ++" test that uses facts on ++).
\end{itemize}

\begin{table}
\resizebox{\textwidth}{!}{
\input{tables/all-lemmas}
}
\caption{A list of the lemmas that were discovered using different vocabularies ($\Vocab$).}
\label{eval:all-lemmas}
\end{table}

\begin{table}[t]
\resizebox{0.9\textwidth}{!}{
\input{tables/experiment-stats}
}

\smallskip
\caption{Statistics for evaluation benchmarks.
In each experiment, we report the total number of lemmas found, as well as candidates that passed the initial screening but failed the prover, and the number of proof retry attempts.
Numbers in parentheses represent faster runs with tweaked parameters (see text).
}
\label{table:experiments}
\end{table}

\subsection{Comparison}
\label{results:comparison}

To compare the performance of \TheSy to that of existing tools, we selected a set of sorted vocabularies that we believe express different aspects and difficulties in theory exploration.
These include different ADTs (lists trees and naturals), lemmas that rely on auxiliary lemmas for their proofs, and conditional lemmas (the latter are currently not supported by \TheSy but are nevertheless a pervasive component in automated theorem proving.)
We compared \TheSy with Hipster~\cite{ITP2017:Johansson}, a theory exploration add-on for Isabelle/HOL, and Leon~\cite{SCALA13:Blanc}, a verification and synthesis tool for Scala.
All three share a domain of purely functional programs with algebraic data types and recursive definitions.
We added Leon to our comparison to see which of the lemmas can be automatically proved without further intervention;
meaning whether exploration can be beneficial and prove or help prove lemmas that are not trivial for it.
As the theory exploration systems stop when they finish exploring defined space, we added a timeout of half an hour for Leon to keep the comparison close.
\autoref{eval:case-study} shows the results of running the three tools on different vocabularies.

Following is a case study comparing the three systems. 
Some cases benefit from using previously obtained knowledge, so results from previous cases are retained.
During the study, Hipster and \TheSy receive only the sorted vocabulary, while in Leon we also add the lemma definitions as it has no concept of theory exploration.
As we can see in the results, there are many cases, usually the more intricate ones, in which the exploration can serve a system like Leon.

\begin{table}
\centering
\resizebox{0.98\textwidth}{!}{%
\input{tables/case-study}
}

\vspace{2pt}
\caption{Results of the case study comparing \TheSy, Hipster, and Leon}
\label{eval:case-study}
\end{table}

\begin{myparagraph}{Case study 1 --- natural numbers.}
We included the canonical construction of natural numbers based on $0$ and $\tsucc$ (successor) and a recursive definition of $+$.
By default, two placeholders are created per type. $+$ has two arguments of the inducted type, requiring three placeholders to fully explore the space.
If only two placeholders were used, commutativity ($x+y=y+x$) can be discovered, but not associativity ($x + (y + z) = (x + y) + z$). With two placeholders, \TheSy can find only limited cases such as $x + (x + y) = (x + x) + y$.
Therefore, the number of placeholders used for this vocabulary is increased to $3$.
\TheSy and Hipster found the same knowledge, with a slight difference.
\TheSy added an extra, unneeded lemma for $x+1$.
It happened due to the ordering of the lemmas, where the smaller is added first.
Leon cannot prove commutativity as it requires an auxiliary lemma; although Leon proved the lemma, it requires user intervention to use it.
\end{myparagraph}

\begin{myparagraph}{Case study 2 - lists.}
This experiment involves polymorphic lists constructed with $[]$ and $\cons$, and the definitions of $\concat$ (list concatenation) and $\trev$ (reverse).
As $\concat$ is present in the vocabulary, with a signature containing the inducted type in more then one argument, similar to Case 1 but here more function symbols are present. 
In this case \TheSy runs twice, building basic knowledge on $\concat$ alone with 3 placeholders and only later runs on the full vocabulary with the default parameters.
In this setting, \TheSy discovers commutativity and associativity of $\concat$, as well as a distributivity with $\trev$ ($\trev~(l_1 \concat l_2) = \trev~l_1 \concat \trev~l_2$) and the nice property $\trev~(\trev~l_1) = l_1$.
Hipster generates most of the expected results but fails to discover $\trev~(\trev~l_1) = l_1$.
Hipster did find a similar lemma, $\trev~(l_1 \concat \trev~l_2) = l_2 ++ \trev~l_1$, which when instantiated with $l_1=[]$, implies the expected lemma. However, requiring this extra reasoning step makes the lemma harder to use in automated provers as well as in interactive environments.
We do note that when we re-ran Hipster with $\Vocab=\{\trev\}$, then, based on lemmas discovered in the previous step, Hipster recovers the missing lemma.
The additional execution leads to a total runtime of 18 minutes.
Leon manages to verify the simple auxiliary lemmas while failing at those that require using them.
\end{myparagraph}

\begin{myparagraph}{Case study 3 --- functional operators.}
With the basic functional primitives for lists $\tmap$ and $\tfilter$, along with $\concat$ from before, \TheSy successfully discovers distributivity properties. Hipster fails to discover these properties.
Hipster can find the comutativity lemma $\tfilter~p~(\tfilter~q~l_1) = \tfilter~q~(\tfilter~p~l_1)$, which \TheSy was unable to reach due to the number of case splits required by $\tfilter$ (as a consequence of the conditional statement in the definition of $\tfilter$).
With $\tfilter$ alone, \TheSy can recover this property in ~7 seconds via an additional case-split depth.
With $\tmap$ and $\concat$ alone, \TheSy finds the same lemmas in ~40 seconds.
This case study shows the importance of automatic case splitting when reasoning about functional programs.
Further research and engineering effort should lead to more efficient treatment of this aspect, enabling a full exploration of theories containing $\tfilter$ and similar functions.
The lemmas found in this case study can be proven without any support lemmas, and Leon succeeded in proving all of them.

We then explored a third functional operator, $\tfold$.
(As an aside, libraries often contain efficient implementations of $\tfold$, so rewriting a user program using $\tfold$ can significantly improve its performance; this was one of the motivations for this case study.)
\TheSy finds four lemmas, one of which is, in fact, redundant (due to an implementation bug).
Hipster finds a single lemma, showing the equality of $\tfold~0~+$ and $\tsum$ but again, includes redundant symbols ($x + (\tsum~y)$ instead of $\tsum~y$).
$\tfold$ is another case where having many placeholders is unfavorable to \TheSy's run time.
If we limit the number of placeholders to one per type, TheSy finishes in 42 seconds.
Leon is unable to prove any lemma in this case.
\end{myparagraph}

\begin{myparagraph}{Case study 4 --- trees.}
An important aspect is dealing with custom data structures.
This case investigates the behaviour over an additional data structure to check the effect on run time. 
All tools succeed in this, although it does not contribute much to the comparison it does demonstrate to the reader the robustness of the exploration.
\end{myparagraph}

\begin{myparagraph}{Case study 5 --- conditions.}
A difficult challenge in theory exploration is creating conjectures containing logical implications, as this quickly leads to space explosion.
By allowing the user to provide predicates to use as premises, it is possible to reduce the lemma space significantly, while retaining suitable conjectures.
\TheSy does not yet support this feature, and therefore, cannot produce such lemmas.
Hipster uses a feature from QuickSpec to generate conjecture based on given predicate \cite{hipstercond}.
This works well in the context of $leq$, but is limited for other forms of predicates (such as \texttt{strstr} as described in \autoref{screening:evaluation}).
\end{myparagraph}
