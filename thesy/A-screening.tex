
\section{Conjecture Inference By Symbolic Expressions}
\label{thesy:screening}

\begin{comment}
Storytelling
\begin{itemize}
    \item Difficulty
        \begin{itemize}
            \item exponential number of terms
            \item n of eq. is quadratic in number of terms
            \item checking equality is generally undecidable, and at any rate, expensive
                 (require induction)
            \item assume no base values (only ADT ctors)
                  --- cannot infer on uninterpreted / non-concrete (recursive) values
        \end{itemize}
    \item Journey
        \begin{itemize}
            \item Use PEG to compactly represent a space of terms, later we will see
              how to explore all conjectures efficiently using memoization
            \item equalities are formed not by testing all pairs, but via equivalence
              reduction; furthermore, equalities that evidently cannot be proved by
              simple induction are pruned away
            \item the equivalence relation is defined symbolically so that it can be
              computed for unknown/incomplete values
            \item inferred rules are used to further prune the space of terms by
              collapsing equivalence classes during iterative deepening.
            \item (the use of PEG helps discover common expressions during symbolic
              evaluation on example values)
        \end{itemize}
\end{itemize}


This section is all about showing 2 problems.
\begin{enumerate}
    \item There are a lot of terms and we want to save on edges
    \item There are even more equality terms and we want to filter to possible ones
\end{enumerate}

Terms as edges of terms to show subliniarity
Mention that by only checking terms and not equality terms there are quardeupaly less terms

\ldots
\end{comment}

This section deals with the first phases of the theory exploration method.
The objective is to find new conjectures using the source code without additional knowledge from the user.
Let us consider a (possibly infinite) language $\Lang$ given by a sorted vocabulary $\Vocab$.
The terms may contain free variables, with the semantics that these will be universally quantified.
For the rest of this paper, equality between terms will be definitional: two terms
$t_1, t_2 \in \Lang$ will be considered equivalent if for any valuation $\sigma$ of the 
free variables to concrete values of the appropriate types, $t_1\sigma$ and $t_2\sigma$
evaluate to the same value.

For practical reasons, SyGuS solvers usually limit generated terms by a height bound $k$.
This serves to bound the space of possible terms, and in our case, has the additional
justification that lemmas containing complex terms will be less general, and therefore,
less useful.
So the benefit of increasing the height drops for each level introduced.
The size of the space, however, grows rapidly: the size of the term space
$\mathcal{T}^{(k)} \eqdef \left\{t ~\vert~ t \in \Lang \land 
    \mathrm{height}(t) \le k\right\}$
is proportional to $\left|\Vocab\right|^k$.
This would be very large even for small vocabularies and small $k$.
Furthermore, the number of equality conjectures is quadratic in the number of terms,
roughly\footnote{this can be reduced by only considering terms of the same type, but with a relatively
small number of term types, will still be asymptotically quadratic.}
$|\mathcal{T}^{(k)}|\cdot\left(|\mathcal{T}^{(k)}-1|\right) = O{\left(|\Vocab|^{2k}\right)}$.
Checking all pairs for equality is clearly intractable.

For this reason, the goal of the screening phase is to effectively approximate the set of
valid equality lemmas and produce a small set of conjectures for the prover to check.
This must be done efficiently while maintaining a low rate of false positives, to avoid overloading
the verifier to the point where the exploration becomes infeasible.
In this section we will describe a technique to reduce the number of checked conjectures without undermining the subsequent steps of the exploration and proof process.

The main idea is to split the terms into equivalence classes whose members \emph{may} be equal for all
possible valuations, by replacing free variables with bounded symbolic values (symbolic expressions), thus over-appoximating
``real'' definitional equality.
We use a compact representation of these equivalence classes, without having to explicitly hold
all the terms in each class.
This helps mitigate the term space explosion and support efficient enumeration and symbolic evaluation
of terms for the purpose of partitioning them into classes.


\subsection{Term Evaluation Methods}
\label{screening:evaluation}

We would like an efficient term comparison procedure that is able to work without additional input from the user and is able to evaluate on all defined types, including function types.
This problem is generally undecidable, therefore solutions will always be incomplete --- meaning that
some equalities will fail to be detected --- with a tradeoff between precision and performance

The general approach taken both here and in \cite{ICAD2013:Claessen} is to enumerate through all terms, and, by evaluating them, find sets of equal terms.
From these sets, equality conjectures are derived and passed to a theorem prover for verification.

Previous work~\cite{JFP2017:Smallbonequickspec2} addressed this by applying random testing with concrete values %\ES{IsaCoSy used counter example generation to do this so less relevant}.
This requires value generators for each type $\Type$ of free variables occurring in the terms being compared, and an execution environment for $\Lang$.
A generator for type $\Type$ is a function $\mathcal{F}_\Type$ generating a random distribution $\mathcal{X}_\Type$ of values from the domain of $\Type$. 
Given that all the relevant $\mathcal{G}_\Type$ are present and they created a sufficient cover of the space of input values, the correct equivalence classes will be found by using Observational Equivalence \cite{CAV2103:Albarghouthi}. %\ES{They talk about observational equivalence in quickspec. is it not enough?}.%

Although each type requires a single generator, the comparison will be limited by the distributions $\mathcal{X}_\Type$. 
The ramification is that a generic value generator may not fit every scenario, and one or more specialized generators are needed for full exploration of different vocabularies.
As an example, consider a function such as C's \texttt{strstr} that searches for an occurrence of a substring within a larger string.
The probability of generating proper substrings with plain random sampling is very low, so with
random testing, it will be hard to distinguish \texttt{strstr} from string equality.
The problem is exacerbated when free variables may take function types, such as is the case when
higher-order functions such as $\textrm{filter}$ are present in the vocabulary.

Therefore, while the random testing approach has been applied successfully before \cite{JFP2017:Smallbonequickspec2,ICAD2013:Claessen,hipstercond,ITP2017:Johansson,2018AISC:Einarsdottir}, it is useful to consider a different approach. 
This work proposes to overcome these issues using a symbolic evaluation method based on term rewriting.
Using semantics-preserving rewrite rules allows to infer that terms are equal without having to
substitute concrete values for the free variables, eliminating the need for random generators.
It also allows to incorporate automatically discovered equality laws as simplification rules.
Obviously, if this inference were complete, we would not even need induction or theory exploration in
the first place.
Since rewrite-based definitional equality is inherently weak, we would not gain much by applying it
directly to the terms with the free variables.
Instead, we do substitute the variables, but with symbolic expressions that, while not representative of
\emph{all} the values of a given type, still represent \emph{infinitely many} such values.
For example, the symbolic expression $[v_1]$, with $v_1:\textrm{int}$, represents all lists of length one,
and $[v_1, v_2]$ --- all the lists of length two.

This method overcomes the limitations of using concrete values by describing (infinitely) many
concrete evaluations at once.
In fact, most if not all of the variables can be left as-is, that is, as uninterpreted values.
Only variables whose values influence the recursion depth need to be restricted.
Consider that by restricting the values relevant to recursion, the rewriting may unroll the terms until equality is found.
For example the term $map~id~[x,y,z]$ could be rewritten by the definitions of $map$ and $id$ into $[x,y,z]$.
The entire evaluation can then be carried out symbolically via a rewrite search, using known equations
$\Eqs$ as the source for rewrite rules $\RewriteSys$ and approximating $\TGroupRelationBounded{}{}{d}$
for some depth parameter $d$.
As mentioned in \autoref{overview:inference}, $\Eqs$ is initialized to the definitions of the functions
of the vocabulary $\Vocab$, but can be extended by discovered equalities, so long as these are proven
correct by \autoref{induction}, thus making the approximation tighter.

On top of that, a deductive approach can take advantage of pre-existing knowledge when analyzing new definitions.
In this way, knowledge collected by analyzing a library or a module can be used when exploring client code that interacts with that library or module.
This can enable reasoning about larger code bases without having to run all the available functions, some of which may perform expensive computations.

\begin{comment}
Also each method or value type used is defined at some time by the user, these definitions are used to create rewrite rules and later further rewrite the terms. 
There are different limitations to this type of system, first of all speed, the system needs to find matches to known rules and works as a high level interpreter. 
Another important limitation is the program space reachable with the rewrite system. 
This rewrite system proves equivalences therefor has the same limitation as provability. 
In addition to the theoretical limit there are unknown and needed rewrite rules for some comparisons, although some will be found in later iterations there is no promise all will be found. 
In order to translate even the simplest terms to equality certain rewrite rules are needed beforehand, for example \lstinline{1 + (x + 1) = (1 + x) + 1}. 
Without having a rewrite about addition being associative this equality will not be found. 
This seems like a huge limitation but as we will see later our system proves the rewrite rules it needs during its process and the function and type definitions provide most needed knowledge. 
As a result providing a set of rewrite rules for the builtins in the language is almost all user knowledge needed.
\end{comment}

\subsection{Generating Candidate Terms}
\label{generation}

In this subsection, we introduce the concept of Syntax Guided Enumeration (SyGuE).
SyGuE is similar to Syntax Guided Synthesis (SyGuS for short~\cite{DSSE2015:Alur}) in that they both use a formal definition of a language to find program terms solving a problem.
They differ in the problem definition: while SyGuS is defined as a search for a correct program over the well-formed programs in the language,
SyGuE is the problem of iterating over \emph{all distinct} programs in the language. SyGuS solvers may be improved using a smart search algorithm, while SyGuE solvers need an efficient way to eliminate duplicate terms, which may depend on the definition of program equivalence.

\begin{definition}[SyGuE]
Given a grammar $G$ and a term equivalence relation $\EquivRel$, the SyGuE problem is defined as generating all the equivalence classes of terms produced by
$G$ modulo $\EquivRel$.

\vspace{8pt}
\centering
$
    \textrm{SyGuE}(G,\EquivRel) := \big\{ \left[t_i\right] \in \Lang/\EquivRel
     ~\vert~ t_i \in \Lang_G \big\}
$
\end{definition}

Since the class $\left[t_i\right]$ may well be infinite, a representative is chosen
to stand for it.
Moreover, it is usually desirable to generate these representative terms in some
ascending order, such as by increasing size or height.
Typically, synthesizers generate terms in layers --- that is, first leaf terms
(height = 0), then iteratively create terms of height = 1, 2, etc. from the terms
of lower heights.
This technique from SyGuS is carried over to SyGuE.

\medskip

As the goal is finding equality conjectures that require induction to prove, we attempt to find all pairs of terms that might be equal.
This is done by creating all the terms and dividing them into equivalence classes using an over-approximation of the ``true'' equivalence relation, $\EquivRel$.
% should we note that finding the true equivalence relation would be great?
% lol yeah. great but hard. lets keep it lik ethis then
%We partition the space of terms
%We make sure each equivalence class contains terms that are equal for the base case of the induction; % I dont get this. I think this came too early in the paper
%look here. it doesn't make sens to write that yet
% why not?
Two terms $t_1$, $t_2$ satisfying $t_1 \EquivRel t_2$ will form a candidate for a new equation.
If we choose the relation $\EquivRel = {\TGroupRelation{}{}}\!$ (which is not an over-approximation), then $t_1$, $t_2$ are provably
equal by $\RewriteSys$.
These are the less interesting equalities: what we are really interested in
are equalities that are not directly provable using $\RewriteSys$,
but can be proven by applying induction.
%here
To this end, $\EquivRel = {\TGroupRelation{}{}}\!$ would be too strong.

Before we define the equivalence relation that we are actually going to use for this instance of SyGuE, we first describe the space of terms that will be generated.
As mentioned in \autoref{overview:inference}, at depth = 0 the synthesizer generates
a collection of placeholders $\pholder{i}{\tau~}$ for the available types $\tau$.
These placeholders will become universally quantified variables in the eventual
equations inferred, but for the time being they are treated as uninterpreted symbols
of the corresponding types.
We store them in the PEG as vertices with nullary edges (similar to $p$, $l_1$, and
$l_2$ in the example of \autoref{screening:representation}).
In addition, we create vertices for all the constants in $\Vocab$.
For each vertex, we also record its type.
These vertices constitute level 0 of the enumeration.

What follows is an iterative deepening process that is common in enumerative synthesis.
For each function symbol, $f\in\Vocab$, of type $\alpha_1\to\alpha_2\to\cdots\to\alpha_k\to\tau$
whose arity is $k$, we go over all the $k$-tuples of vertices $\langle u_1, \cdots, u_k\rangle$
such that $u_i : \alpha_i$ according to the recorded types.
We then generate a new vertex $v$ with associated type $\tau$, and the hyperedge
$\langle u_1, \cdots, u_k\rangle\xrightarrow{f}v$.
(As a common optimization, to avoid recreating terms at higher levels, we also require that
at least one of $u_1,\cdots,u_k$ be new, that is, generated at the previous level.)

\begin{comment}

To understand the creation of all terms into a compact structure, first how the terms are created is discussed, then we consider how they are merged, which is the next step in the process. To create all terms in the given language, special rewrite rules and accompanying hyper graph are created. Consider a grammar $\mathcal{G}$ of identifiers $i_j$ and accompanying types $t_j$. We separate identifiers by type, between functions and values. Starting with the hyper graph, all values in the grammar are inserted into an empty graph with their accompanying types. With the hyper graph ready we can build deeper terms using the function symbols from the grammar. Each function symbol is translated into a rewrite rule, matching parameters from the existing graph by type and resulting in a new vertex in the hyper graph, representing the created term. Practically in addition to the initial values created from the grammar, values for each type appearing in the grammar are generated, which we will refer to as placeholder values. Their purpose is to make sure all terms are created and are later on used for the equivalence step as is explained in section 2.3. For simplicity, for each type two placeholder values are created marked as $V_{p_i}$.

\begin{equation}
    \mathcal{G} := \left(\left\{v_1: t_1, v_2: t_2, \ldots, v_n: t_n\right\}, \left\{f_1: t'_1, f_2: t'_2, \ldots, f_m: t'_m\right\}\right)
\end{equation}

 

Similar to Observational Equivalence (OE), the term space is being pruned by finding equalities before creating deeper terms. Normally Observational Equivalence prunes the space by keeping only one representative from each equivalence class. In this work it is important to keep all the terms, therefor equal terms are merged in the compact structure and each vertex represents all the terms in the equivalence class. This way we gain the pruning without losing the current and future terms. Future terms refers to terms that would not have been considered in normal enumeration using OE. Let's consider the terms $x + y$ and $y + x$, they will be found equal and only one will be kept, for the example $x + y$. Future terms being created will be based on this, $1 + (x + y)$. The problem is we do not consider $1 + (y + x)$, which might make the system miss needed conjectures. Using Symbolic Observational Equivalence, both terms are kept but the new term is not yet constructed. Therefor we create only one new hyper term which will look like $1 + t \vert t \in \{x + y, y + x\}$.

There are two important points that contribute to the size of the equivalence classes in the graph. First is the merge step, which creates the equivalence classes, all the terms in a class are merged as will be explained in Section 2.3, meaning they are represented by a single vertex of the hyper graph. Later in the process the system will try to prove new lemmas, which translate to rewrite rules, these rules will be used to further merge terms. As creating the terms is done in steps, until the needed depth, the result of merging terms in the previous step will apply when creating the next level of terms. When matching the grammar rewrite rules, there are less vertexes to according to the merges that happened. Now it is not necessary to iterate all the terms created in order to produce the deeper terms, only the vertexes, which as we will see later, represent the equivalence classes.

Algorithm for term generation
\begin{enumerate}
    \item Create uninterpreted values - $val\_a_1$,$val\_a_2$, $val\_list\_a_1$, $val\_list\_a_2$ 
    \item Iterate until term depth reached
    \begin{enumerate}
        \item Iterate grammar rewrite rules
        \begin{enumerate}
            \item apply rewrite rule to hyper graph
        \end{enumerate}
        \item Use equality function to merge vertexes
    \end{enumerate}
\end{enumerate}

\ES{Choose example to run with. Show that many programs are the same like (1+1)+1 = 1+(1+1)}
\end{comment}

\subsection{Conjecture Inference}
\label{screening:soe}

%Computing equivalence classes from a given set of rewrite rules $\RewriteSys$ was described (\autoref{screening:evaluation}) $\SERelation{}{}$.

Now we describe the first part of how equivalence classes are formed and how this is used to find conjectures. Remember that our goal is to find equivalences that cannot be found in the current search space.
This cannot be done directly with the uninterpreted values $\pholder{i}{\tau}$ created above,
since most rewrite rules will have to \emph{destructure} at least one of the arguments to a function.
For example, the term $x \concat y$ has no reduxes w.r.t. the rewrites induced by the definition
of $\concat$ given in \autoref{overview:input-state}.
To overcome this, we need to introduce some structure into the values represented by the placeholders.

We generate small \emph{symbolic expressions} to be used as \emph{symbolic examples}.
They are created from the recursive data types used as placeholder types
by combining uninterpreted values from the base types and constructors.
These symbolic examples are used to discriminate programs according to their functional behavior. 
Programs that evaluate to the same (symbolic) value on all the examples are merged in the PEG. 
We take advantage of the compaction mechanism already present in the PEG data structure to detect when
symbolic values are equal, without ever having to construct those values explicitly or define a normal form.
We repeat this process for a set of possible symbolic valuations to the placeholders: for each valuation,
essentially, a working copy of the PEG is created, placeholders are filled in, and rewrites are applied up to the rewrite depth $d$.
Whenever two vertices $u_1$, $u_2$ were successfully merged in all of these derived PEGs, we merge them
in the master PEG as well to indicate that they are now considered equivalent.
This creates an ever-growing equivalence relation over the set $\Terms$ of terms generated so far.

\begin{comment}
\SI{This should go somewhere else probably}
{\color{gray}
Before explaining how the examples are generated and used, we define Rewrite Search. Rewrite Search is how we explore the term search space, by iteratively applying the known rewrite rules. After applying all the rewrite rules in iteration $i$, all proofs of depth $i$ will have been found. This means all term equalities provable with $i$ steps will be expressed as merges in the compact structure. There is no point of matching rewrite rules on the same parts of the graph twice. To prevent this we use a versioned graph and for each iteration match at least one part of the rewrite rule with the newest version of the graph.
}
\end{comment}

It would be tremendously expensive, and also unnecessary, to create all the combination of symbolic values (that would be exponential in the number of placeholders.)
Instead, we found that it is sufficient, sometimes even desirable, to only expand a single placeholder.
For the purpose of presentation, assume there is a single type $\tau$ (e.g. list) over which we plan to
perform structural induction.
We associate symbolic values with placeholder $\pholder{1}{\tau~}$, e.g.
$\pholder{1}{\tlist~T} \mapsto \left\{[], v_1\cons [], v_2\cons v_1\cons []\right\}$.
The remaining placeholders remain uninterpreted.
When multiple algebraic data types are present, the process is just repeated for each of them.

\begin{definition}[Symbolic Observational Equivalence]
\label{screening:serelation}
Given a set $\mathcal{S}_{\tau}\subseteq\Lang$ of terms representing symbolic values of type $\tau$,
We define the \emph{symbolic observational equivalence (SOE)} over terms as

\vspace{2pt}
\centering
$\SERelation{t_1}{t_2} \iff \bigwedge_{s\in\mathcal{S}_{\tau}} 
    \GroupRelationBounded{\substInTerm{t_1}{s}{\pholder{1}{\tau~}}}
                         {\substInTerm{t_2}{s}{\pholder{1}{\tau~}}}{d}$
\end{definition}

That is, then the symbolic values of $t_1$ and $t_2$ converge for all the symbolic values $\mathcal{S}_{\tau}$ that $\pholder{1}{\tau~}$ can take.
Equality between symbolic values is determined by a rewrite search of depth $d$.

\begin{paragraph}{Automatic case-splitting.~~}
Even with a single induction variable, and with size-bounded symbolic values,
symbolic evaluation can get blocked and cripple the equality checks.
This is a known phenomenon stemming from the presence of conditional branches
in definitions.
For example, in the expression $\tfilter~p~(v_1\cons {[]})$, the definition
of $\tfilter$ would introduce a term of the form
``$\tmatch~(p\,v_1)~\twith~\cdots$''.
Evaluation cannot continue without knowing the value of $p\,v_1$.
The rewrite-based comparator overcomes this using a split-case, in a way
that is commonly employed by symbolic execution environments~\cite{OSDI2008:Cadar,CACM2013:Cadar}
Evaluation is \emph{forked} and both branches explored, where each fork
assumes an additional \emph{path condition}:
\textit{e.g.}, one fork assumes $p\,v_1=\tfalse$, and the other
$p\,v_1=\tfalse$. Two terms will then be merged only if they are merged
on both forks.

Evidently, the number of forked states grows exponentially with the number
of conditions that may be encountered during evaluation.
It is important to apply split-case judiciously, so a tighter bound is 
placed on the number of nested forking steps than the bound $d$ on
regular rewrite steps.
Luckily, a small number of split is sufficient in many scenarios.
Consider the following two terms from the running example in \autoref{overview:inference} \eqref{overview:example-terms}.
\[
\tfilter~\pholder{1}{T\to\tbool}~
(\pholder{1}{\tlist\,T} \concat \pholder{2}{\tlist\,T})
\qquad
(\tfilter~\pholder{1}{T\to\tbool}~\pholder{1}{\tlist\,T})
~\concat ~
(\tfilter~\pholder{1}{T\to\tbool}~\pholder{2}{\tlist\,T})
\]

So the screener will have to compare, \textit{e.g.},
\[
\tfilter~p~\big((v_1\cons{[]}) \concat l_2\big) ~~\maybeEq~~
{\big(\tfilter~p~(v_1\cons{[]})\big)} \concat {(\tfilter~p~l_2)}
\]

While $\tfilter$ would eventually apply $p$ to all the elements of
both $v_1\cons{[]}$ and $l_2$, it is enough to split into two cases:
when $p\,v_1=\ttrue$, both terms evaluate to
$v_1 \cons {\tfilter~p~l_2}$; when $p\,v_1=\tfalse$, they both evaluate to
$\tfilter~p~l_2$.

\end{paragraph}

\medskip
% This concludes this section
At the end of each iteration, every vertex represents an equivalence class defined by the rewrite search modulo the set of symbolic valuations that was used.
At this point we can say that all combination of two terms from a single class can be an equality conjecture, which still has to be checked by the prover since the symbolic examples still do not cover \emph{all} possible inputs.
This leads to the next part of the process where we further filter the conjectures and try to prove the ones left using induction.
