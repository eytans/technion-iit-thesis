
\label{thesy:intro}

\begin{comment}
Move 1
Computer science and formal methods  Reduce to a mathematical logic problem, which is usually solved based on a set of known lemmas. (Leon, Dafny (SMT), Compiler optimization, equivalence reduction in synthesis).
Methods to collect lemmas - Interactive provers, Automic provers, Interactive Theory Exploration (Knowledge), Automatic Theory Exploration.

Move 2
Non theory exploration methods suck cause they are not offline
Current theory exploration relies on testing and does not scale to complicated types or data structures (Must show scaling in TheSy). Examples where current doesn't find lemmas on functional theories when only based on functional languages.

Move 3
Step 1 a
Create a general theory exploration technique capable of treating functions as first class objects.
Should not be limited to a specific logic or language.
Step 1 b 
Fast and robust theory exploration technique
Definition for knwoledge
Definition for SOE

Step 2
Finds general lemmas (as opposed to hipster).
Faster.
Scales.

Motivation build up
Start with a verification problem
Dafny reduces it to a logic problem (proof obligation)
(Working style) The solution will require the user writing multiple layers of lemmas until Dafny is able to solve.
(Build up to "production") To get to the general "system" lemmas the user (or automatic search) will have to go very deep in the deconstruction of the original obligation.
This is time consuming as there are numerous potential lemmas
* Not all correct lemmas are helpful and not all helpful lemmas are correct.
The user needs specific understanding and expertise to build a correct proof (with or without automation).
 
Let's meet in the middle (offline proving)
Put automatic proof search, hipster in
Explain why hipster sucks

Main contributions:
* Thesy is implemented to be awsome (including all things in motivation)
* We did SOE which should be the new OE in orange
* Created benchmark for high-order theory exploration and evaluated new and existing benchmarks speed and lemma usefulness vs hipster (maybe isacosy)


% Intro By Hila's advice
Popular (or many or some) software verification tools, such as Dafny \cite{} and Leon \cite{}, use support lemmas 


% Motivation
Logical deduction is fundamental to automatic reasoning and, therefore, to software verification. 
A verification task is typically reduced to a proof obligation; after which, the obligation is dismissed by automatic reasoning techniques.
Most of these techniques rely on logical inference-rules to apply logical deduction, and attempt to construct a proof. As proof construction might fail,
state of the art verification tools [Dafny, Leon, Cvc4] support the input of lemmas which can then be applied using generic inference rules.
Supplying correct and suiting lemmas is time consuming, and requires low-level understanding of the verified code.

% Establishing a territory
For many cases, automatic theorem provers discover auxiliary lemmas as part of the proof search.
Depending on the underlying logic and theory, support lemmas might be an essential part of any pro of for a given theorem.
Discovering lemmas during proof search can be resource expensive, as a lemma can be any valid formula, leading to infinite possibilities.

Two main discovery approaches exist, top down and bottom up.
In top down discovery [vampire, e prover, cvc4, acl2], the prover starts with a proof obligation, and heuristically guides the search using it; during the search it attempts to find and prove lemmas.
While existing provers are capable of intricate proofs, lemma discovery 
Common automatic theorem proving techniques are based on superposition calculus [Vampire, and a few more], machine learning and search [e prover, hammers], and SMT solvers [Cvc4 and friends]. All these tools require a proof obligation from which the search is guided. This is called top down proving, as the search starts from the obligation. While awsome, creating lemmas for Dafny and Leon and freinds will be manual.
A different approach, bottom up proving, relies on lemma discovery, and proves all lemmas that are found, expanding the underlying theory.
While less suited for theorem proving, as the goal does not guide the lemm search. it was shown to perform well in such tasks [hipspec]. It is definitly more suited for what we want to do.
\end{comment}

% General centrality
Most forms of software verification and synthesis rely on some form of logical reasoning to complete their task.
Whether it is checking pre- and post-conditions, deriving specifications for sub-problems~\cite{PLDI2015:Feser,POPL2016:Albarghouthi}, or equivalence reduction~\cite{VMCAI2019:Smith}, these methods rely on assumptions from both the input and relevant background knowledge.
Domain-specific knowledge can reinforce these methods, whether via the design of a domain-specific language~\cite{PLDI2001:Xiong,HPEC2000:Moura,CACM2018:Ragan-Kelley}, specialized decision procedures~\cite{TODAES2012:Milder}, or decomposing specifications~\cite{ACM2015:Polozov}.
While hand-crafted techniques can treat whole classes of programs, every library or module contributes a collection of new primitives, requiring tweaking or extending these methods.
Automatic formation of background knowledge can enable effortless treatment of such libraries and programs.

% Motivation - Intro to theorem proving scalability 
% (Gap 1 - scalability)
In the context of verification tools, such as Dafny~\cite{LPAIR2010:Rustan} and Leon~\cite{SCALA13:Blanc}, as well as interactive proof assistants, such as 
Coq~\cite{Coq:manual} and Isabelle/HOL~\cite{Book2002:Nipkow},
background knowledge is typically given as a set of \emph{lemmas}.
Usually, these libraries of lemmas (\ie the background knowledge) are created by human engineers and researchers who are tasked with formulating them and proving their correctness.
When a proof or verification task requires auxiliary lemmas missing from the existing background knowledge, 
the user is required to add and prove it, sometimes repeating this process until the proof is trivial or can be found automatically.
For example, both Dafny and Leon fail to prove that addition is associative and commutative from first principles---based on an algebraic construction of the natural numbers.
However, when given knowledge of these properties (\ie encoded as lemmas: $(x + y) + z = x + (y + z)$ and $x + y = y + x$)%
\footnote{In fact, these properties are hard-wired into decision procedures for linear integer arithmetic in SMT solvers.}, they readily prove composite facts such as
$(x + 5) + y = 5 + (x + y)$.

%Lemmas are truths that follow logically from definitions and axioms, meaning they might be found mechanically; 
%automated provers are not keenly adept at doing so.

\begin{comment}
Auxiliary propositions, such as the aforementioned properties of addition, are usually stored as part of one or more libraries
that are utilized by a reasoning tool such as a prover or a compiler.
Notable examples for this are the standard libraries of popular proof assistants
and the rewrite rules used by the Haskell compiler GHC for
performing optimizations such as Stream Fusion~\cite{SIGPLAN-Notices2007:Coutts}.
These libraries are written by expert developers over the course of years, and tend to grow quite large, making them hard to use and to maintain.
\end{comment}

% Intro to theory exploration
A possible solution is to eagerly generate valid lemmas, and to do so automatically, offline,
as a precursor to any work that would be built on top of the library.
This paradigm is known as \emph{theory exploration}~\cite{Journal2002:Buchberger,JAL2006:Buchberger}, and differs from the common conjecture generation approach (in theorem provers and SMT solvers~\cite{cvc4induction}) that is guided by a proof goal.
As opposed to using proof goal as the basis for discovering sub-goals, when eagerly generating lemmas there is a vast space of possible lemmas to consider.
Currently, two main approaches exist for filtering candidate conjectures, counterexample-based and observational equivalence-based~\cite{JAR2010:Johanssonisacosy,2018AISC:Einarsdottir,hipster,hipstercond}.
These filtering techniques are all based on testing and therefore require automatic creation of concrete examples.

% Gap 2 - concrete examples suck
Testing with concrete values allows for fast evaluation and filtering of terms when the data types involved are simple.
However, when scaling to larger data types and function types it becomes a bottleneck of the theory exploration process.
Previous research effort has revealed that testing-based discovery is sensitive to the number and size of type definitions occurring in the code base.
For example, QuickSpec, which is based on QuickCheck (as are all the existing testing-based theory exploration methods), employs a heuristic to restrict the set of types allowed in terms in order to make the checker's job easier.
Compound data types such as lists can be nested up to two levels (lists of lists, but not lists of lists of lists).
This presents an obstacle towards scaling the approach to real software libraries, since ``\textit{QuickCheck's size control
interacts badly with deeply nested types} [...] \textit{will generate extremely large test data.}''~\cite{JFP2017:Smallbone}

Following are two example scenarios that attempt to represent cases from software systems where structured data types and complicated APIs exist:
(i)~A series of tree data-types $T_i$ where each $T_i$ is a tree of height i with i children of type $T_{i-1}$, and the base case is an empty tree.
Creating concrete examples for $T_i$ will be resource expensive, as each tree has $O(i!)$ nodes, and each node requires a value.
(ii)~An ADT (Algebraic Data Type) $A$ with multiple fields where each can contain a large amount of text or other ADTs, and a function over $A$ that only accesses one of the fields.
Even if evaluating the function is fast, fully creating $A$ is expensive and will impact the theory exploration run-time.

%A common solution is to use abstraction to simplify ‘reasoning’. One possibility is to abstract the domain of possible values. ‘Possibly add examples of such abstractions’.
This paper presents a new symbolic theory exploration approach that takes advantage of the characteristics of induction-based proofs.
To overcome the blowup in the space of possible values, we make use of \emph{symbolic values}, which contain interpreted symbols, uninterpreted symbols, or a mixture of the two. 
Conceptually, each symbolic value is an abstraction representing (infinitely) many possible values. 
This means that preexisting knowledge on the symbolic value can be applied without fully creating interpreted values.
Still, when necessary, uninterpreted values can be expanded, creating larger symbolic values, thus refining the abstraction, and facilitating the necessary computation.
We focus on the formation of \emph{equational} theories, that is, lemmas that curtail the equivalence of two terms, with universal quantification over all free variables.

\begin{comment}
Auxiliary propositions, such as the aforementioned properties of addition, are usually stored as part of one or more libraries
that are utilized by a reasoning tool such as a prover or a compiler.
Notable examples for this are the standard libraries of popular proof assistants
and the rewrite rules used by the Haskell compiler GHC for
performing optimizations such as Stream Fusion~\cite{SIGPLAN-Notices2007:Coutts}.
These libraries are written by expert developers over the course of years, and tend to grow quite large, making them hard to use and to maintain.


\ES{Not a necessary paragraph}
Standard libraries, esp. ones included with proof assistants,
tend to include a collection of useful lemmas for functions defined therein.
As the vocabulary of types and functions grows, the number of such axioms in the library may grow polynomially,
since many useful lemmas involve two or more
function symbols from it.
Even if proving each individual lemma is easy --- indeed, modern proof
assistants offer sufficient automation to make constructing some of these
routine proofs almost trivial --- merely writing down all the possible
lemmas is a tall order.
Moreover, writing a variant of an existing function, even a very simple
one, may bring an avalanche of new lemmas that must be proved for the
new variant w.r.t. all the other functions,
including correspondence between the variant and the original function.
As a result, theories contained in libraries are most often incomplete, and
lemmas are added to them as the need arises, \textit{e.g.} as they emerge
in the course of developing larger proofs that use some definitions
from said library.
\end{comment}

% We are so cool (filling the gap)

%The concept of theory exploration was shown to be useful by HipSpec~\cite{CAD2013:Claessen}, specifically for equational reasoning.

We show that our symbolic method for theory exploration is more applicable and faster in many different scenarios than state-of-the-art.
As an example, given standard definitions for the list functions: $\clrterm\concat~\tdrop~\ttake~\tfilter$
our method proves facts that were not found by current state-of-the-art such as:

\centerline{$
\renewcommand\arraystretch{1.3}
\begin{array}{c}
{(\ttake~i~\mathit{xs})}~\concat~{(\tdrop~i~\mathit{xs})} = \mathit{xs} \\
\tfilter~p~(\mathit{xs}\,\concat\,\mathit{ys}) = (\tfilter~p~\mathit{xs})~\concat~(\tfilter~p~\mathit{ys})
\end{array}
$}
%

\medskip
\begin{myparagraph}{Main contributions}
This paper provides the following contributions:
%\ES{We didn't contribute anything else? A method for comparing uninterpreted terms? Maybe split method for theory synthesis and implementation, Evalation method for theory exploration techniques? we need to check hipspec, isaplanner and isascheme to see how they compared}
\begin{itemize}[topsep=2pt, leftmargin=1.5em]
%    \item A definition of a \emph{knowledge base}, and what it means to be a \emph{knowledge extension} relative to an underlying reasoning method.
%    New knowledge is said to extend exiting knowledge if it can be used as an enabler for deriving new conclusions using the same underlying reasoning, conclusions that could not be reached using the initial knowledge base.
%    This provides a conceptual framework for theory exploration through use of formal methods.
    \item A system for \emph{theory synthesis} using symbolic values to take advantage of value abstraction.
    Our implementation, \TheSy, can discover more lemmas than were found by testing-based tools, while being faster in most cases.
    \item A technique to compare universally quantified terms using term rewriting techniques and a given set of lemmas, called \emph{symbolic observational equivalence} (SOE). 
    SOE overapproximates term equalities deducible by the given lemmas (\ie, will find more equalities), thus can be used for equality reduction in context of uninterpreted values, enabling fully symbolic reasoning over a large set of terms.
    %\item \ES{SOE}Definition of an interesting sub-problem, \emph{Syntax-Guided enumeration} (SyGuE) that emerges from our treatment of lemma discovery, but is useful for synthesis tasks as well as verification (such as for quantifier instantiation and invariant inference).
    %Broadly, it refers to an exhaustive generation of programs defined by a given vocabulary, modulo a custom equivalence relation---a form of pruning known as \emph{equivalence reduction}.
    \item An evaluation of our theory exploration system on a set of benchmarks for induction proofs taken from CVC4~\cite{cvc4induction} and TIP 2015~\cite{CICM2015:Claessen}, specifically the IsaPlanner benchmarks~\cite{ITP2010:Johansson}.
    We compare our implementation with a current leading theory exploration system, Hipster~\cite{2018AISC:Einarsdottir}, using a novel metric. This metric is insensitive to the amount of found lemmas, but rather measures their usefulness in the context of theorem proving.
\end{itemize}
\end{myparagraph}