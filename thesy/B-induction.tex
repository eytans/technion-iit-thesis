\section{Induction by Deduction}
\label{thesy:induction}

This section describes the final conjecture filtering and proof attempt process.
After running Symbolic Observational Equivalence (\autoref{screening:soe}), equivalence classes created from symbolic examples are obtained.
Terms that were found equal by the approximation (using rewriting) of $\SERelation{}{}$ might be provably equal without requiring induction.
At this point it is needed to create finer classes by a relation unifying conjectures that are provable without induction.

Our experiments show that this filtering passes through a small set of candidate lemmas.
We use the same rewrite search to discharge the induction step.
While simple, we found it to be quite robust when integrated with the exploration system as a whole.
Instead of guessing auxiliary lemmas that are needed to carry a proof to
completion, we rely on discovery of provable lemmas during search.
The robustness also stems from an ordering of the conjectures being explored.

\subsection{Refining Equivalence Relation}

Given a set of terms $\Terms$, we start by building an over approximation of the provable term equalities. 
This is done by approximating the equivalence class $C \in \Terms / \SERelation{}{}$ (see \autoref{screening:serelation}).
As mentioned, this step is meant to screen out conjectures that do not require induction to prove, before applying induction, which can be done given the current set of rewrite rules $\RewriteSys$.
This screening helps in preventing addition of unneeded rewrite rules into $\RewriteSys$, keeping it minimal and reducing run time.

We screen the conjectures by taking all the terms in $C$ and create a finer partitioning $C_F = C / \underset{C}{\xleftrightarrow[]{\RewriteSys}}^{\!(d)}$.
Note that as the equivalence classes are represented in the compact structure (\autoref{screening:representation}), they are all represented by a single vertex.
Therefore, to be able to perform rewrite search and find the finer classes, it is needed to reconstruct the terms into a new compact representation, where each term is represented by a different vertex.
We still wish to reduce the amount of terms we enumerate; although it does not affect the asymptotic complexity of the problem, it results in significant speedups.
The reconstruction is done bottom-up, taking advantage of memoization for common subterms (which are numerous), reducing the amount of edges that have to be inserted to the graph.

\newcommand\tgood{\textit{good}}
\newcommand\tbad{\textit{bad}}

By creating $C_F$ using a rewrite search, we take advantage of the fact that running a rewrite search and finding that $t_1 = t_2$ constitutes as a proof.
This is true when uninterpreted values are used, as is the case in while approximating $C_F$.
From the definition of $C_F$ all terms in $\left[ t_j \right] \in C_F$ are proven to be equal to each other by the rewrite search.
For each such $\left[ t_j \right]$ we want to choose a single representative.
This representative term will be used in creating final conjectures to be passed to the prover.
It actually matters which representative we choose, although they were proven equal.
Consider the conjecture $c_{\tgood} := x + y = y + x$, we could have chosen different representatives and write it as $c_{\tbad} := 0 + x + y = 0 + y + x$.
The chosen representative will affect the created rewrite rule.
Choosing the larger term might lead to a rewrite rule that won't match all needed contexts while applied.

At this phase our main concern is to choose the representatives and order the conjectures well. 
We want to avoid having both rules $c_{\tgood}, c_{\tbad}$ are proven, or worse, having $c_{\tbad}$ proven and $c_{\tgood}$ discarded because it is redundant.
Continuing with the example, if $c_{\tbad}$ was proved before $c_{\tgood}$, it means $c_{\tbad}$ is now represented in $\RewriteSys$.
During a screening, $c_{\tgood}$ will be deemed unnecessary due to the presence of $c_{\tbad}$.
This can happen since the following is a valid rewrite chain:
\[x + y \xrightarrow{x = 0 + x} 0 + x + y \xrightarrow{c_{\tbad}} 0 + y + x \xrightarrow{y = 0 + y} y + x\] 

Because the system has a feedback loop, during which the lemmas proved are used again for later proofs, it is important to avoid such bad choices.

We define ordering on terms in an effort to always choose first the term that will match the most situations. 
We define order over the terms first by size (smallest first), then by generality (most variables first).
\begin{definition}[Term Ordering]
\label{induction:ordering}
For a term $t\in\Lang$, let $|t|$ denote its size, that is, the number of
AST nodes in its syntactic representation.
We define a preference ordering $\preceq$ of terms lexicographically over
the tuple $\big\langle|t|, |\FV(t)|\big\rangle$, where term sizes are compared with $\leq$
and free variable sets sizes are compared with $\geq$.
\[t_1 \preceq t_2 \iff \big\langle|t_1|, |\FV(t_1)|\big\rangle
    \underset{\scriptscriptstyle (\leq,\geq)}{\leq}
    \big\langle|t_2|, |\FV(t_2)|\big\rangle\]
\end{definition}
We then use this ordering as a heuristic to choose a representative from each $\left[ t_j \right] \in C_F$, which will be minimal according this order.

The overall process can be described as running a $SyGuE(G, \SERelation{}{})$
followed by $SyGuE(G_{[t_j]}, \GroupRelationBounded{}{}{d})$
for each of the equivalence classes, where $G_{[t_j]}$ is the input grammar restricted to the equivalence class $[t_j]$.
Finally, enumerating the resulting representatives is much less than all the pairs in each $[t_j]$
and definitely much less than $|\Terms|^2$.

\subsection{Induction Step}

The last step of each iteration in this process is attempting to prove the conjecture using induction.
Given the equivalence class $C$ and representatives for each fine equivalence class $\left[ t_j \right] \in C_F$, a conjecture is built as $c := t_1 = t_2$ for each combination two representatives.
As will be shown in the results, only a small amount $\left[ t_j \right]$ classes exist foreach $C_F$, which goes to show this screening technique is effective. 
For each conjecture $c$ we already know at this point that $t_1$ is \emph{not} provably equal to $t_2$ in $d$ rewrite steps.
This is the point in the process that we assume that an induction proof is needed. Each conjecture is used as the induction hypothesis of the proof;
intuitively, if there is a need for a stronger induction hypothesis,
we rely on the system itself to eventually discover a sufficient auxiliary lemma.
\begin{comment}
at the past or at the future the system will prove it as a lemma and that knowledge will become available for the induction step.
Still, some lemmas only will obviously be impossible to find with this limitation, for example using a precondition $x > y$, $max(x,y) \quad \overrightarrow{x > y} \quad x$. Currently such a condition will need to arise from how functions are defined by in the given input. 
\end{comment}

At this point only the induction step is needed.
The reason for that is that base cases for the induction proof were already taken care of in \autoref{screening:soe}. 
This happens when the examples are proved to be equal, which include all the base cases of the inductive data type. It remains to carry out the induction step.

Given the conjecture $c := t_1 = t_2$, during induction on type $\Type$, there is a need for induction step proof for each constructor $\ctor_k(v_1, \ldots, v_{n_k})$ of $\Type$.
We construct a search space representing the proof obligation for $\ctor_k$, to be dismissed using rewrite search.
The search space is just the compact structure containing the terms $t_1, t_2$ \footnote{Recall that some of the rewrite rules are directed, requiring that both terms be included.}, which are updated to represent a step using $\ctor_k$.
The induction variable will always be chosen as the first placeholder value for the type $\Type$ which we will refer to as the induction variable or $iv$.
A term $t$ will be updated by substituting $t[\ctor_k(v_1, \ldots, v_{n_k})/iv]$ where the constructor parameters are new uninterpreted variables of the appropriate types.
We define a well founded order $\ltwf$, to automatically recognise terms that are sub-structures of $iv$ to
which the induction hypothesis can be applied soundly.
For example, given $iv = v_1 \cons v_2$ (for $v_1 : \Type$, $v_2 : \tlist\,\Type$), we have $v_2 \ltwf iv$.
Both the hypothesis and the order $\ltwf$ are represented as rewrite rules added into $\RewriteSys$.
Some proofs will require that the inducted value will be in a certain position in the term, \textit{e.g.} the first or the second argument
to a recursive function. 
This is solved ahead of time by generating all possible terms, as we also generate a term with the placeholder in the right position for the induction.

%Even if the proof will succeed sometimes it is not enough.
The order of the conjectures being tested can affect the results of the theorem search.
Consider plus associativity, the system can find that $c_1 := x + (y + z) = (x + y) + z$ and also that $c_2 := x + (y + x) = (x + y) + x$.
If we succeed in proving $c_2$ before $c_1$, we will end up with an unneeded lemma.
However, by proving $c_1$ first and adding it to $\RewriteSys$, we can dismiss $c_2$ as it can be obtained from $c_1$ without induction.
The first step to solving this is using the order defined in \autoref{induction:ordering}.
Another problem that may arise is proofs failing due to insufficient rewrite rules.
Take the example of $\tfold$ and $\tsum$ (\autoref{induction:foldsum}).
The following three conjectures are at play:
%
\begin{align}
\label{induction:conjfoldsum}    
 c_1 &~:= & \tfold ~ (+) ~ 0 ~ l &= \tsum ~ l     & & \\
 c_2 &~:= & x + \tfold ~ (+) ~ 0 ~ l &= \tfold ~ (+) ~ x ~ l  & & \nonumber \\
 c_3 &~:= & x + \tfold ~ (+) ~ 0 ~ l &= x + sum ~ l   & & \nonumber
\end{align}

Before proving the conjecture $c_1$, the conjecture $c_2$ needs be proved.
Due to the ordering, first the smaller conjecture's proof, i.e. $c_1$ will be attempted and will fail, then $c_2$ will be proved.
Without any further rearrangement of the conjectures, a proof attempt of conjecture $c_3$ will succeed. 
This makes the system find that $c_1$ is unnecessary and will prevent the system from proving it.
\begin{comment}
\ES{We've already been through such an explanation so seems redundant to put this here}
This happens because during the screening phase both
$\tfold ~ (+) ~ 0 ~ \pholder{1}{\tlist\,\tnat}$ and 
$0 + \big(\tfold ~ (+) ~ 0 ~ {\pholder{1}{\tlist\,\tnat}}\big)$ 
are in the same fine equivalence class.
\end{comment}

Order is very important in this system, therefor if a lemma is found, all conjecture proofs that were failed to this point are reattempted.
Before reattempting proof for a conjecture $c$, we recheck if the conjecture $c$ is still needed by running a rewrite search over the terms of $c$.
This way the system returns to the original smaller conjecture and succeeds in proving it finding \iterm{$\tfold~(+)~0~l = \tsum~l$}, making 
$c_3$ redundant, thus proving only the smallest needed theorems for this case.

This proof technique is robust as it can work for user defined functions and data structures, while proving the needed auxiliary lemmas on the run. 
Although most of the proof process is dependant on the previous steps, the induction proof is independent of it's predecessors. 
This means that any proof technique that can work with two given terms can be used instead. 
Having said that the naive approach solved all the needed proofs and does not majorly affect the run time. 
As only a few term combinations reach this stage the run time of a single proof search is of little affect on the total system run time.

\begin{example}{Proof of \eqref{induction:conjfoldsum}$c_1$ --- $\tfold ~ (+) ~ 0 ~ l = \tsum ~ l$ by induction on $l$, once \eqref{induction:conjfoldsum}$c_2$ has been proven.}
\label{induction:foldsum}
\vspace{-3mm}
\[
\renewcommand{\arraystretch}{1.3}
\newcommand\xs{\mathit{xs}}
\begin{array}{l@{\quad}l}
\textit{Assume} & 
\tfold~(+)~0~\xs = \tsum~\xs
 \\[.5em]
\textit{Induction Step}
    & \tfold ~ (+) ~ 0 ~ (x\cons\xs) \Rightarrow \tfold ~ (+) ~ (0 + x) ~ \xs 
      \Rightarrow \tfold ~ (+) ~ x ~ \xs \\
    & ~~\overset{c_2\,}{\Longrightarrow}  x + \tfold ~ (+) ~ 0 ~ \xs \overset{\textrm{IH}\,}{\Longrightarrow}
    x + \tsum ~ \xs \Rightarrow \tsum~(x\cons \xs) \\[-.75em]
\square &
\end{array}
\]

Notice that this example, variants of which occur in some tutorials on
formal proofs using proof assistants, typically requires a strengthening
of the inductive property.
This was completely avoided here thanks to the auxiliary lemma, which was automatically discovered.
\end{example}

\begin{comment}
\begin{example}{Proof of $\reverse(l \snoc x) = x :: \reverse(l)$ (Equation \eqref{induction:conjfoldsum}) by
induction on $l$.}
\renewcommand{\arraystretch}{1.2}
\[
\begin{array}{lll}
    \textnormal{Base Step:} &l& = nil \\
    && \reverse(nil \snoc x) \Rightarrow \reverse(x :: nil) \Rightarrow 
       nil \snoc x \\
    &&\Rightarrow  x :: nil \Rightarrow x :: \reverse(nil) \\[.5em]
    \textnormal{Induction Step:} &l& = y::l_1 \\
    && \reverse(y :: l_1 \snoc x) \Rightarrow \reverse(l_1 :+ x) \snoc y \\
    && \Rightarrow x :: \reverse(l_1) \snoc y \Rightarrow x :: \reverse(y :: l_1)
\\
\end{array}
\]
Note: The rewrites used are only from the definitions of the functions ($\snoc$ and $\reverse$).
\end{example}
\end{comment}